{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "'''\n",
    "\"\"\"\n",
    "1.대화 말뭉치 파일을 읽어들인다.\n",
    "2.대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "3.Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "4.딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "5.자연어 이해 모델을 관리한다(저장,읽기)\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "# komoran = Komoran()\n",
    "twitter = Okt()\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialog_nlp_processing(intents):\n",
    "    \"\"\"\n",
    "     대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?']\n",
    "    # loop through each sentence in our intents patterns\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each word in the sentence\n",
    "#             w = nltk.word_tokenize(pattern)\n",
    "            pos_result = twitter.pos(pattern, norm=True, stem=True)\n",
    "            w = [lex for lex, pos in pos_result]\n",
    "            # add to our words list\n",
    "            words.extend(w)\n",
    "            # add to documents in our corpus\n",
    "            documents.append((w, intent['tag']))\n",
    "            # add to our classes list\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "    \n",
    "    # stem and lower each word and remove duplicates\n",
    "#     words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "#     words = sorted(list(set(words)))\n",
    "    words = [w for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "    \n",
    "    # remove duplicates\n",
    "    classes = sorted(list(set(classes)))\n",
    "    \n",
    "    return classes, documents, words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_machine_learning(classes, documents, words):\n",
    "    \"\"\"\n",
    "    Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "    \"\"\"\n",
    "    \n",
    "    # create our training data\n",
    "    training = []\n",
    "    output_row = []\n",
    "    # create an empty array for our output\n",
    "    output_empty = [0] * len(classes)\n",
    "    \n",
    "    # training set, bag of words for each sentence\n",
    "    for doc in documents:\n",
    "        # initialize our bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        pattern_words = doc[0]\n",
    "        # stem each word\n",
    "#         pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "        # create our bag of words array\n",
    "        for w in words:\n",
    "            bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "        # output is a '0' for each tag and '1' for current tag\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "        training.append([bag, output_row])\n",
    "    \n",
    "    # shuffle our features and turn into np.array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training)\n",
    "    \n",
    "    # create train and test lists\n",
    "    train_x = list(training[:,0])\n",
    "    train_y = list(training[:,1])\n",
    "\n",
    "    return train_x, train_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorflow_learning_model(train_x, train_y, output_model_file_name):\n",
    "    \"\"\"\n",
    "    딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    # reset underlying graph data\n",
    "    #tf.compat.v1.reset_default_graph()\n",
    "    tf.reset_default_graph()\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "    \n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir='home_tflearn_kr_logs')\n",
    "    # Start training (apply gradient descent algorithm)\n",
    "    model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "\n",
    "#     # Using Keras, Build neural network\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Dense(8, input_shape=(len(train_x[0]),)),\n",
    "#         tf.keras.layers.Dense(8),\n",
    "#         tf.keras.layers.Dense(len(train_y[0]), activation=\"softmax\"),\n",
    "#         ])\n",
    "    \n",
    "#     model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#     model.fit(train_x, train_y, epochs=1000, batch_size=8)    \n",
    "\n",
    "    # save the trained model to directory\n",
    "    model.save(output_model_file_name)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "def save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name):\n",
    "    \"\"\"\n",
    "    자연어 이해 모델을 관리한다(저장,읽기)\n",
    "    \"\"\"\n",
    "    # save all of our data structures\n",
    "    pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( output_training_data_file_name, \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 documents\n",
      "9 classes ['Slang', 'food', 'goodbye', 'greeting', 'market', 'mission', 'parking', 'parking_resp', 'thanks']\n",
      "67 unique stemmed words ['.', 'marekt', 'mission', '가다', '감사', '감사하다', '거기', '계세', '계시다', '고맙다', '구역', '그렇다', '꺼지다', '나중', '날', '누구', '니까', '다가', '다음', '닥치다', '돈', '들리다', '또', '마켓', '많이', '매장', '먹다', '멍청이', '메가박스', '무엇', '미션', '바보', '버세', '번', '사라지다', '새끼', '서비스', '수고', '식당', '식당가', '아쿠아리움', '안녕', '안녕하다', '야', '어디', '에', '에요', '여기', '요', '은', '음식', '음식점', '이', '이다', '임마', '입', '있다', '자다', '장사', '좋다', '주차', '지내다', '찾기', '추천', '친절하다', '하다', '해주다']\n"
     ]
    }
   ],
   "source": [
    "# 대화 말뭉치 파일을 읽어들인다.\n",
    "input_file_name = './DialogIntents/intents_shop.json'\n",
    "intents = read_dialog_intents_jsonfile(input_file_name)\n",
    "    \n",
    "# 대화 말뭉치를 읽어서 자연어 처리 및  Bag of word 생성\n",
    "classes, documents, words = dialog_nlp_processing(intents)\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjszz\\.conda\\envs\\chat_env\\lib\\site-packages\\ipykernel_launcher.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# Bag of word를 딥러닝 알고리즘 활용을 위한 입력으로 변환\n",
    "train_x, train_y = prepare_machine_learning(classes, documents, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6999  | total loss: \u001b[1m\u001b[32m0.01068\u001b[0m\u001b[0m | time: 0.023s\n",
      "| Adam | epoch: 1000 | loss: 0.01068 - acc: 0.9999 -- iter: 48/52\n",
      "Training Step: 7000  | total loss: \u001b[1m\u001b[32m0.01050\u001b[0m\u001b[0m | time: 0.027s\n",
      "| Adam | epoch: 1000 | loss: 0.01050 - acc: 0.9999 -- iter: 52/52\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\sjszz\\GroupProject\\step3\\jupyter_app\\nlp\\home_chat\\NLUModel\\model_home_kr.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝(tensorflow)을 통한 자연어 이해 모델 생성\n",
    "output_model_file_name = './NLUModel/model_home_kr.tflearn'\n",
    "model = create_tensorflow_learning_model(train_x, train_y, output_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DNN' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DNN' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 이해 모델을 관리한다(저장,읽기)\n",
    "output_training_data_file_name = \"./NLUModel/training_data_shop\"\n",
    "save_training_data_structures(words, classes, train_x, train_y, output_training_data_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is Bag of word for '매장' :[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "classes :['Slang', 'food', 'goodbye', 'greeting', 'market', 'mission', 'parking', 'parking_resp', 'thanks']\n",
      "model.predict([p]) :[[2.7431198e-15 4.4267382e-19 7.2726714e-10 5.9709069e-03 9.9213260e-01\n",
      "  1.1590638e-07 6.1150274e-18 3.7271653e-13 1.8964186e-03]]\n",
      "52 documents\n",
      "9 classes ['Slang', 'food', 'goodbye', 'greeting', 'market', 'mission', 'parking', 'parking_resp', 'thanks']\n",
      "67 unique stemmed words ['.', 'marekt', 'mission', '가다', '감사', '감사하다', '거기', '계세', '계시다', '고맙다', '구역', '그렇다', '꺼지다', '나중', '날', '누구', '니까', '다가', '다음', '닥치다', '돈', '들리다', '또', '마켓', '많이', '매장', '먹다', '멍청이', '메가박스', '무엇', '미션', '바보', '버세', '번', '사라지다', '새끼', '서비스', '수고', '식당', '식당가', '아쿠아리움', '안녕', '안녕하다', '야', '어디', '에', '에요', '여기', '요', '은', '음식', '음식점', '이', '이다', '임마', '입', '있다', '자다', '장사', '좋다', '주차', '지내다', '찾기', '추천', '친절하다', '하다', '해주다']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"매장\", words)\n",
    "print(\"p is Bag of word for '매장' :{}\".format(p))\n",
    "print(\"classes :{}\".format(classes))\n",
    "    \n",
    "print(\"model.predict([p]) :{}\".format(model.predict([p])))\n",
    "    \n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatApp_env",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
